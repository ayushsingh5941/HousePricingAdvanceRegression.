import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import category_encoders as ce
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_log_error
import xgboost as xgb

# Importing data
data_train = pd.read_csv('train.csv')
data_test = pd.read_csv('test.csv')
Id = data_test['Id']
data_target = data_train['SalePrice']

# Finding correlation
corr_matrix = data_train.corr()
# correlation matrix graph
print('Correlation', corr_matrix['SalePrice'].sort_values(ascending=False))

# Visualizing sales price distribution
sns.distplot(data_train['SalePrice'])

# Exploring sale price
print("Skewness = ", data_train['SalePrice'].skew())
print("kurtosis = ", data_train['SalePrice'].kurt())


# Promising labels are GrLivArea, TotalBmsf, oveallQual and year built
# plotting graph for GrLivArea


def relation_plot():
    var = 'GrLivArea'
    df = pd.concat([data_train['SalePrice'], data_train[var]], axis=1)
    df.plot.scatter(x=var, y='SalePrice', ylim=(0, 800000))

    var = 'TotalBsmtSF'
    df = pd.concat([data_train['SalePrice'], data_train[var]], axis=1)
    df.plot.scatter(x=var, y='SalePrice', ylim=(0, 800000))

    # box plot overallqual/saleprice
    var = 'OverallQual'
    data = pd.concat([data_train['SalePrice'], data_train[var]], axis=1)
    f, ax = plt.subplots(figsize=(8, 6))
    fig = sns.boxplot(x=var, y="SalePrice", data=data)
    fig.axis(ymin=0, ymax=800000)

    var = 'YearBuilt'
    data = pd.concat([data_train['SalePrice'], data_train[var]], axis=1)
    f, ax = plt.subplots(figsize=(16, 8))
    fig = sns.boxplot(x=var, y="SalePrice", data=data)
    fig.axis(ymin=0, ymax=800000);
    plt.xticks(rotation=90);


# missing data
total = data_train.isnull().sum().sort_values(ascending=False)
percent = (data_train.isnull().sum() / data_train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
print(missing_data.head(20))


# Removing target
data_train.drop('SalePrice', axis=1, inplace=True)

# Basic data familiarization
print('Training data shape :', data_train.shape)

# Splitting data
x_train, x_valid, y_train, y_valid = train_test_split(data_train, data_target, test_size=0.25, random_state=42)


# pre-processing and feature engineering


def feature_engineering(data):
    # Removing unwanted columns
    discarded_columns = ['Alley', 'Id', 'PoolArea', 'MoSold', '3SsnPorch', 'MiscVal', 'MSSubClass', 'KitchenAbvGr',
                         'MiscFeature']
    data.drop(discarded_columns, axis=1, inplace=True)
    # Filling object with NA
    for columned in data.columns:
        if data[columned].dtype == 'object':
            data[columned].fillna(data[columned].mode(), inplace=True)

    # Filling int and float columns with median
    for columned in data.columns:

        if data[columned].dtype == 'int64' or data[columned].dtype == 'float64':
            median = data[columned].median()
            data[columned].fillna(median, inplace=True)

    # Adding new features taking log of variable data
    data['LotFrontageLog'] = np.log(data['LotFrontage'])
    data['LotAreaLog'] = np.log(data['LotArea'])
    data['1stFlrSF'] = np.log(data['1stFlrSF'])
    data['GrLivAreaLog'] = np.log(data['GrLivArea'])
    data['PropertyAge'] = (2019 - data['YearBuilt'])
    data['SoldAge'] = (2019 - data['YrSold'])
    # Property remodeled age
    return data


x_train_featured = feature_engineering(x_train.copy())
x_valid_featured = feature_engineering(x_valid.copy())
x_test_featured = feature_engineering(data_test.copy())
"""
# Fetching cols for categorical encoding
cols = []
for columns in x_train_featured.columns:
    if x_train_featured[columns].dtype == 'object':
        cols.append(columns)

# Encoding categorical data
cat_enc = ce.OneHotEncoder(cols=cols)
pipe = Pipeline(steps=[('cat', cat_enc)])

x_train_encoded = pipe.fit_transform(x_train_featured)
x_valid_encoded = pipe.transform(x_valid_featured)
x_test_encoded = pipe.transform(x_test_featured)


# random forest and selecting features
rf_class = RandomForestRegressor(random_state=42, n_estimators=100)
sel = SelectFromModel(rf_class)
sel.fit(x_train_encoded, y_train)
selected_feature = x_train_encoded.columns[(sel.get_support())]
print(selected_feature)
x_train_selected_feature = x_train_encoded.loc[:, x_train_encoded.columns.intersection(selected_feature)]
x_valid_selected_feature = x_valid_encoded.loc[:, x_valid_encoded.columns.intersection(selected_feature)]
x_test_selected_feature = x_test_encoded.loc[:, x_test_encoded.columns.intersection(selected_feature)]
print('shape of selected features', x_train_selected_feature.shape)

# Pca and Standard scaler with pipeline
pca = PCA(n_components=0.99)
sc = StandardScaler()
pipe = Pipeline(steps=[('sc', sc), ('pca', pca)])
x_train_piped = pipe.fit_transform(x_train_selected_feature)
x_valid_piped = pipe.transform(x_valid_selected_feature)
x_test_piped = pipe.transform(x_test_selected_feature)


# model to train
xgboost = xgb.XGBRegressor(random_state=42, objective='reg:squarederror')
xgboost.fit(x_train_piped, y_train)
"""
"""
grid_param = {'n_estimators': [10, 100, 1000],
              'max_depth': [80, 90, 100, 110],
              'min_samples_split': [8, 10, 12],
              'bootstrap': [True, False]}
grid = GridSearchCV(rf_class, grid_param, cv=3, n_jobs=-1)
grid.fit(x_train_piped, y_train)
print('Best param', grid.best_params_)
model = grid.best_estimator_
model.fit(x_train_piped, y_train)
"""
"""
predict = xgboost.predict(x_valid_piped)
print('RMLSE grid search', np.sqrt(mean_squared_log_error(y_valid, predict)))
# Training neural network for prediction

# Predicting and Saving result on test set
y_pred = xgboost.predict(x_test_piped)

output = pd.DataFrame({'Id': Id,
                       'SalePrice': y_pred})

output.to_csv(r'submission.csv', index=False)
"""
plt.show()
